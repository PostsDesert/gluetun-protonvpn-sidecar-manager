services:
  # 0. Network Anchor (Holds the namespace and ports)
  network-anchor:
    image: alpine:latest
    container_name: ${VPN_INSTANCE_NAME:-proton}-network-anchor
    command: tail -f /dev/null
    ports:
      - ${ANCHOR_PORT:-0}:8000/tcp # Gluetun Control
    restart: always

  # 1. The Gateway (Gluetun + ProtonVPN)
  gluetun:
    image: qmcgaw/gluetun
    container_name: ${VPN_INSTANCE_NAME:-proton}-gluetun
    network_mode: service:network-anchor
    depends_on:
      - network-anchor
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    env_file:
      - ${ENV_FILE_NAME:-.env}
    environment:
      # Provider Config - Switched to CUSTOM to use IP/Key directly
      - VPN_SERVICE_PROVIDER=custom
      - VPN_TYPE=wireguard
      
      # Connection Details (Managed by Sidecar)
      - WIREGUARD_ENDPOINT_IP=${WIREGUARD_ENDPOINT_IP}
      - WIREGUARD_ENDPOINT_PORT=${WIREGUARD_ENDPOINT_PORT}
      - WIREGUARD_PUBLIC_KEY=${WIREGUARD_PUBLIC_KEY}
      
      # Authentication (From your Proton config)
      # You must generate a WireGuard configuration from ProtonVPN dashboard
      # and set these values in your .env file or hardcode them here.
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - WIREGUARD_ADDRESSES=${WIREGUARD_ADDRESSES:-10.2.0.2/32}
      
      # DNS & Firewall
      - DNS_SERVER=on
      - BLOCK_MALICIOUS=on
      - BLOCK_SURVEILLANCE=on
      - BLOCK_ADS=on
      - DNS_UPSTREAM_RESOLVERS=cloudflare,google
      
      # IMPORTANT: Force safe MTU to prevent connection instability
      - WIREGUARD_MTU=1280  
    volumes:
      - gluetun-data:/gluetun
    restart: always

  # 2. The Exit Node (Tailscale + Headscale)
  tailscale-vpn:
    image: tailscale/tailscale:latest
    container_name: ${VPN_INSTANCE_NAME:-proton}-exit-node
    network_mode: service:network-anchor # Routes all traffic through Anchor
    depends_on:
      - network-anchor
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    volumes:
      - ./tailscale-data:/var/lib/tailscale # Persist state (login)
      - /dev/net/tun:/dev/net/tun
    environment:
      # Auth Key from your Headscale/Tailscale server
      # Generate a REUSABLE key so the container can survive recreates
      - TS_AUTHKEY=${TS_AUTHKEY}
      
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_EXTRA_ARGS=--advertise-exit-node --login-server=${TS_LOGIN_SERVER}
      - TS_HOSTNAME=${VPN_INSTANCE_NAME:-proton}-exit-node
      - TS_ACCEPT_DNS=false
      # Force Kernel Networking Mode for better performance
      - TS_USERSPACE=false
    restart: always

  # 3. Sidecar Manager (Monitors Load & Health)
  vpn-manager:
    build: .
    container_name: ${VPN_INSTANCE_NAME:-proton}-manager
    environment:
      - TARGET_CITIES=${TARGET_CITIES:-San Jose}
      - TARGET_COUNTRY=${TARGET_COUNTRY}
      - HEALTH_CHECK_INTERVAL=${HEALTH_CHECK_INTERVAL:-60}
      - LOAD_CHECK_INTERVAL=${LOAD_CHECK_INTERVAL:-900}
      # Auth credentials for Proton API
      - PROTON_USERNAME=${PROTON_USERNAME}
      - PROTON_PASSWORD=${PROTON_PASSWORD}
      
      # Docker Compose Control
      - COMPOSE_PROJECT_NAME=${COMPOSE_PROJECT_NAME:-tailscale-proton}
      - GLUETUN_CONTAINER_NAME=${VPN_INSTANCE_NAME:-proton}-gluetun
      - GLUETUN_SERVICE_NAME=gluetun
      - ENV_FILE_PATH=/project/${ENV_FILE_NAME:-.env}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # Check/Restart containers
      - .:/project # Access to .env file
      - ./proton-session:/data # Persist session cache
    restart: always

  # 4. Network Configurator (Fixes routing & firewall for Tailscale <-> Gluetun)
  configurator:
    image: alpine:latest
    container_name: ${VPN_INSTANCE_NAME:-proton}-configurator
    network_mode: service:network-anchor
    cap_add:
      - NET_ADMIN
    depends_on:
      - tailscale-vpn
    command: >
      sh -c "apk add --no-cache iptables iproute2 &&
      echo 'Waiting for interfaces...' &&
      while ! ip link show tun0 >/dev/null 2>&1; do sleep 1; done &&
      while ! ip link show tailscale0 >/dev/null 2>&1; do sleep 1; done &&
      
      apply_rules() {
        echo 'Applying routing fixes...'
        ip rule add to 100.64.0.0/10 lookup 52 priority 98 2>/dev/null || true
        ip rule add from 100.64.0.0/10 lookup 51820 priority 102 2>/dev/null || true
        
        echo 'Applying firewall rules...'
        # Clear existing rules to prevent duplicates
        iptables -D FORWARD -i tailscale0 -o tun0 -j ACCEPT 2>/dev/null || true
        iptables -D FORWARD -i tun0 -o tailscale0 -m state --state RELATED,ESTABLISHED -j ACCEPT 2>/dev/null || true
        iptables -t nat -D POSTROUTING -o tun0 -j MASQUERADE 2>/dev/null || true
        iptables -t mangle -D FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu 2>/dev/null || true
        
        # Apply new rules
        iptables -I FORWARD 1 -i tailscale0 -o tun0 -j ACCEPT
        iptables -I FORWARD 1 -i tun0 -o tailscale0 -m state --state RELATED,ESTABLISHED -j ACCEPT
        iptables -t nat -I POSTROUTING 1 -o tun0 -j MASQUERADE
        iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu
      }
      
      apply_rules
      
      echo 'Network configured successfully! Entering monitoring loop...'
      while true; do
        # Re-apply if tun0 disappears and comes back (Gluetun restart)
        if ! ip link show tun0 >/dev/null 2>&1; then
          echo 'tun0 lost! Waiting for it to return...'
          while ! ip link show tun0 >/dev/null 2>&1; do sleep 1; done
          echo 'tun0 returned. Re-applying rules...'
          apply_rules
        fi
        sleep 10
      done"
    restart: always

volumes:
  gluetun-data:
